{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/tobias/.local/lib/python3.8/site-packages (1.23.1)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/tobias/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/tobias/.local/lib/python3.8/site-packages (from pandas) (1.23.1)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.4.3 pytz-2022.1\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tobias/.local/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tobias/.local/lib/python3.8/site-packages (from matplotlib) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/tobias/.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /home/tobias/.local/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Installing collected packages: pillow, cycler, kiwisolver, fonttools, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.34.4 kiwisolver-1.4.3 matplotlib-3.5.2 pillow-9.2.0\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "Requirement already satisfied: packaging in /home/tobias/.local/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Processing /home/tobias/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501/termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/tobias/.local/lib/python3.8/site-packages (from tensorflow) (1.23.1)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Using cached typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Using cached wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/tobias/.local/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.34.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.22.0)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.9.0-py2.py3-none-any.whl (167 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Using cached importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.1)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.1.0)\n",
      "Installing collected packages: termcolor, libclang, protobuf, h5py, keras-preprocessing, keras, astunparse, gast, google-pasta, grpcio, flatbuffers, tensorflow-io-gcs-filesystem, absl-py, typing-extensions, wrapt, tensorflow-estimator, opt-einsum, importlib-metadata, markdown, werkzeug, tensorboard-plugin-wit, rsa, cachetools, google-auth, requests-oauthlib, google-auth-oauthlib, tensorboard-data-server, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.1.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.9.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.47.0 h5py-3.7.0 importlib-metadata-4.12.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 opt-einsum-3.3.0 protobuf-3.19.4 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 typing-extensions-4.3.0 werkzeug-2.1.2 wrapt-1.14.1\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "path_to_file = 'shakespear.txt'\n",
    "text = open(path_to_file, 'r').read()\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {char:i for i, char in enumerate(vocab)}\n",
    "ind_to_char = np.array(vocab)\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 180\n",
    "total_num_seq = len(text)//(seq_len+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "P\n",
      "r\n",
      "o\n",
      "j\n",
      "e\n",
      "c\n",
      "t\n",
      " \n",
      "G\n",
      "u\n",
      "t\n",
      "e\n",
      "n\n",
      "b\n",
      "e\n",
      "r\n",
      "g\n",
      " \n",
      "e\n",
      "B\n",
      "o\n",
      "o\n",
      "k\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "C\n",
      "o\n",
      "m\n",
      "p\n",
      "l\n",
      "e\n",
      "t\n",
      "e\n",
      " \n",
      "W\n",
      "o\n",
      "r\n",
      "k\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "W\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "a\n",
      "m\n",
      " \n",
      "S\n",
      "h\n",
      "a\n",
      "k\n",
      "e\n",
      "s\n",
      "p\n",
      "e\n",
      "a\n",
      "r\n",
      "e\n",
      ",\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "W\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "a\n",
      "m\n",
      " \n",
      "S\n",
      "h\n",
      "a\n",
      "k\n",
      "e\n",
      "s\n",
      "p\n",
      "e\n",
      "a\n",
      "r\n",
      "e\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "T\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "e\n",
      "B\n",
      "o\n",
      "o\n",
      "k\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      "n\n",
      "y\n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "y\n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "U\n",
      "n\n",
      "i\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "S\n",
      "t\n",
      "a\n",
      "t\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      "\n",
      "\n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "o\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "t\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      " \n",
      "c\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "a\n",
      "l\n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      "r\n",
      "i\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      "\n",
      "\n",
      "w\n",
      "h\n",
      "a\n",
      "t\n",
      "s\n",
      "o\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      ".\n",
      " \n",
      "Y\n",
      "o\n",
      "u\n",
      " \n",
      "m\n",
      "a\n",
      "y\n",
      " \n",
      "c\n",
      "o\n",
      "p\n",
      "y\n",
      " \n",
      "i\n",
      "t\n",
      ",\n",
      " \n",
      "g\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "a\n",
      "w\n",
      "a\n",
      "y\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "r\n",
      "e\n",
      "-\n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "e\n",
      "r\n",
      "m\n",
      "s\n",
      "\n",
      "\n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "P\n",
      "r\n",
      "o\n",
      "j\n",
      "e\n",
      "c\n",
      "t\n",
      " \n",
      "G\n",
      "u\n",
      "t\n",
      "e\n",
      "n\n",
      "b\n",
      "e\n",
      "r\n",
      "g\n",
      " \n",
      "L\n",
      "i\n",
      "c\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "l\n",
      "u\n",
      "d\n",
      "e\n",
      "d\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "e\n",
      "B\n",
      "o\n",
      "o\n",
      "k\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "o\n",
      "n\n",
      "l\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "a\n",
      "t\n",
      "\n",
      "\n",
      "w\n",
      "w\n",
      "w\n",
      ".\n",
      "g\n",
      "u\n",
      "t\n",
      "e\n",
      "n\n",
      "b\n",
      "e\n",
      "r\n",
      "g\n",
      ".\n",
      "o\n",
      "r\n",
      "g\n",
      ".\n",
      " \n",
      "I\n",
      "f\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "l\n",
      "o\n",
      "c\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "U\n",
      "n\n",
      "i\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "S\n",
      "t\n",
      "a\n",
      "t\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      "\n",
      "\n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "c\n",
      "h\n",
      "e\n",
      "c\n",
      "k\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "w\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "n\n",
      "t\n",
      "r\n",
      "y\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "# Create Training Sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "for i in char_dataset.take(500):\n",
    "     print(ind_to_char[i.numpy()])\n",
    "\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "    \n",
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106  49  68  65   2  45  78  75  70  65  63  80   2  36  81  80  65  74\n",
      "  62  65  78  67   2  65  31  75  75  71   2  75  66   2  49  68  65   2\n",
      "  32  75  73  76  72  65  80  65   2  52  75  78  71  79   2  75  66   2\n",
      "  52  69  72  72  69  61  73   2  48  68  61  71  65  79  76  65  61  78\n",
      "  65  13   2  62  85   2  52  69  72  72  69  61  73   2  48  68  61  71\n",
      "  65  79  76  65  61  78  65   1   1  49  68  69  79   2  65  31  75  75\n",
      "  71   2  69  79   2  66  75  78   2  80  68  65   2  81  79  65   2  75\n",
      "  66   2  61  74  85  75  74  65   2  61  74  85  83  68  65  78  65   2\n",
      "  69  74   2  80  68  65   2  50  74  69  80  65  64   2  48  80  61  80\n",
      "  65  79   2  61  74  64   1  73  75  79  80   2  75  80  68  65  78   2]\n",
      "﻿The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other \n",
      "\n",
      "\n",
      "[49 68 65  2 45 78 75 70 65 63 80  2 36 81 80 65 74 62 65 78 67  2 65 31\n",
      " 75 75 71  2 75 66  2 49 68 65  2 32 75 73 76 72 65 80 65  2 52 75 78 71\n",
      " 79  2 75 66  2 52 69 72 72 69 61 73  2 48 68 61 71 65 79 76 65 61 78 65\n",
      " 13  2 62 85  2 52 69 72 72 69 61 73  2 48 68 61 71 65 79 76 65 61 78 65\n",
      "  1  1 49 68 69 79  2 65 31 75 75 71  2 69 79  2 66 75 78  2 80 68 65  2\n",
      " 81 79 65  2 75 66  2 61 74 85 75 74 65  2 61 74 85 83 68 65 78 65  2 69\n",
      " 74  2 80 68 65  2 50 74 69 80 65 64  2 48 80 61 80 65 79  2 61 74 64  1\n",
      " 73 75 79 80  2 75 80 68 65 78  2 76]\n",
      "The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other p\n"
     ]
    }
   ],
   "source": [
    "for input_txt, target_txt in  dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    # There is an extra whitespace!\n",
    "    print(''.join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
    "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\n",
    "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    model.add(LSTM(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "model = create_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embed_dim=embed_dim,\n",
    "  rnn_neurons=rnn_neurons,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 22:09:19.993609: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 4526 of 10000\n",
      "2022-07-10 22:09:29.993131: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 9361 of 10000\n",
      "2022-07-10 22:09:31.398242: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n",
      "2022-07-10 22:09:41.871475: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 94556160 exceeds 10% of free system memory.\n",
      "2022-07-10 22:09:42.487382: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 94556160 exceeds 10% of free system memory.\n",
      "2022-07-10 22:09:47.087382: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 94556160 exceeds 10% of free system memory.\n",
      "2022-07-10 22:09:48.252218: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 94556160 exceeds 10% of free system memory.\n",
      "2022-07-10 22:09:48.627219: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 94556160 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 2585s 11s/step - loss: 2.8651\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-10 22:52:21.459769: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 9525 of 10000\n",
      "2022-07-10 22:52:21.987172: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 [==============================] - 2585s 11s/step - loss: 2.1346\n",
      "Epoch 3/30\n",
      "239/239 [==============================] - 2303s 10s/step - loss: 1.8774\n",
      "Epoch 4/30\n",
      "239/239 [==============================] - 2176s 9s/step - loss: 1.7128\n",
      "Epoch 5/30\n",
      "239/239 [==============================] - 2076s 9s/step - loss: 1.5910\n",
      "Epoch 6/30\n",
      "239/239 [==============================] - 2126s 9s/step - loss: 1.5067\n",
      "Epoch 7/30\n",
      "239/239 [==============================] - 2126s 9s/step - loss: 1.4460\n",
      "Epoch 8/30\n",
      "239/239 [==============================] - 2102s 9s/step - loss: 1.4001\n",
      "Epoch 9/30\n",
      "239/239 [==============================] - 2133s 9s/step - loss: 1.3650\n",
      "Epoch 10/30\n",
      "239/239 [==============================] - 2098s 9s/step - loss: 1.3363\n",
      "Epoch 11/30\n",
      "239/239 [==============================] - 2062s 9s/step - loss: 1.3129\n",
      "Epoch 12/30\n",
      "239/239 [==============================] - 2046s 9s/step - loss: 1.2931\n",
      "Epoch 13/30\n",
      "239/239 [==============================] - 2034s 8s/step - loss: 1.2762\n",
      "Epoch 14/30\n",
      "239/239 [==============================] - 2035s 8s/step - loss: 1.2609\n",
      "Epoch 15/30\n",
      "239/239 [==============================] - 2075s 9s/step - loss: 1.2470\n",
      "Epoch 16/30\n",
      "239/239 [==============================] - 2042s 9s/step - loss: 1.2344\n",
      "Epoch 17/30\n",
      "239/239 [==============================] - 1962s 8s/step - loss: 1.2234\n",
      "Epoch 18/30\n",
      "239/239 [==============================] - 2088s 9s/step - loss: 1.2121\n",
      "Epoch 19/30\n",
      "239/239 [==============================] - 2070s 9s/step - loss: 1.2017\n",
      "Epoch 20/30\n",
      "239/239 [==============================] - 2124s 9s/step - loss: 1.1923\n",
      "Epoch 21/30\n",
      "239/239 [==============================] - 2058s 9s/step - loss: 1.1826\n",
      "Epoch 22/30\n",
      "239/239 [==============================] - 2082s 9s/step - loss: 1.1739\n",
      "Epoch 23/30\n",
      "239/239 [==============================] - 2131s 9s/step - loss: 1.1647\n",
      "Epoch 24/30\n",
      "239/239 [==============================] - 2056s 9s/step - loss: 1.1562\n",
      "Epoch 25/30\n",
      "239/239 [==============================] - 2138s 9s/step - loss: 1.1482\n",
      "Epoch 26/30\n",
      "239/239 [==============================] - 2141s 9s/step - loss: 1.1398\n",
      "Epoch 27/30\n",
      "239/239 [==============================] - 2080s 9s/step - loss: 1.1318\n",
      "Epoch 28/30\n",
      "239/239 [==============================] - 2084s 9s/step - loss: 1.1240\n",
      "Epoch 29/30\n",
      "239/239 [==============================] - 2164s 9s/step - loss: 1.1162\n",
      "Epoch 30/30\n",
      "239/239 [==============================] - 2166s 9s/step - loss: 1.1086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fa189fdc0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "epochs = 30\n",
    "model.fit(dataset,epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou shall not pass my master.\n",
      "\n",
      "CLOWN.\n",
      "Well, beseech you, sir, if thou dost give me the natural propose, ’tis above agains\n",
      "thee.\n",
      "\n",
      " Enter Benvoline,\n",
      "Or two Partans feede? Welcome him welcome,\n",
      "And hang behind who you do learn these presses,\n",
      "That we will mean time to the Appliant.\n",
      "Here comes the chair will have all thy pains,\n",
      "That thou with turn the laws of thy bride.\n",
      "\n",
      "Here comes an old fortune; and the cause would flourish\n",
      "On this appearan, plain and writting sorrow,\n",
      "Who, guilting scorns, his liberal praise,\n",
      "He meant to have his dreams, and birds in shade.\n",
      "\n",
      "THESEUS.\n",
      "This resolve more is the part of attendants;—\n",
      "But now let it mistook him justice\n",
      "To smythe Timon's fatal watchen attemn'd with the former parl,\n",
      "    So diest. The almighty pledge and moons\n",
      "    Which, swaming, piss'd and rash, not prize,\n",
      "    Nothing except, all best stand afar,\n",
      "  But whom we come to be an east given to\n",
      "    me's againe.\n",
      "  CORIOLANUS. Will th' eleven blossom, will de of the sons.\n",
      "  YORK. I never seeming, northed, thou art Lion, who\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('shakespeare_gen.h5') \n",
    "\n",
    "#Currently our model only expects 128 sequences at a time. We can create a new model that only expects a batch_size=1. We can create a new model with this batch size, then load our saved models weights.\n",
    "#Then call .build() on the mode\n",
    "\n",
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "model.load_weights('shakespeare_gen.h5')\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "\n",
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "  time series problems.\n",
    "  '''\n",
    "  # Number of characters to generate\n",
    "  num_generate = gen_size\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char_to_ind[s] for s in start_seed]\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to effect probability of next characters.\n",
    "  # Higher probability == lesss surprising/ more expected\n",
    "  # Lower temperature == more surprising / less expected\n",
    "  temperature = temp\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(ind_to_char[predicted_id])\n",
    "  return (start_seed + ''.join(text_generated))\n",
    "\n",
    "\n",
    "print(generate_text(model,\"thou shall not pass\",gen_size=1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
