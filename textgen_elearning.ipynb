{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the crawled texts\n",
    "## Converting JSON to plain text\n",
    "At first, the JSON output of the spider is converted into plaintext. This is achieved by splitting the strings in the JSON into seperate lines unsing `split()` and the combine them unsing `join()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json = \"webcrawler/biology.json\" # Where webcrawler output lives\n",
    "with open(path_to_json, 'r') as fr:\n",
    "    pre_ = fr.read() # read JSON file\n",
    "    lines = pre_.split('\\n') # split text into seperate lines\n",
    "    new_filename = path_to_json.split('.')[0]+\".txt\" # To keep the same name except ext\n",
    "    with open(new_filename, \"a\") as fw:\n",
    "        fw.write(\"\\n\".join(lines)) # join lines together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plain text file of the same filename is saved in the directory of the JSON file.\n",
    "\n",
    "## Cleaning the plain text for model building\n",
    "\n",
    "The steps described here are based on the following tutorials:\n",
    "* [Text Cleaning for NLP: A Tutorial](https://monkeylearn.com/blog/text-cleaning/)\n",
    "* [Pandas dataframe, German vocabulary – select words by matching a few 3-char-grams – I](https://linux-blog.anracom.com/2021/09/04/pandas-dataframe-german-vocabulary-select-words-by-matching-a-few-3-char-grams-i/)\n",
    "\n",
    "### Step 1: Text Normalization\n",
    "\n",
    "Text normalization aims at easing the computers understanding of the text at hand. For instance, we commonly use capitalizations and other special characters, which might interfere with model building.\n",
    "\n",
    "If not normalized, our machine would intepret \"Hello\" differently than \"hello\" which doesn't really matter. On the other hand - especially in German language which we will be dealing with here - missing capitalization might interfere with our understanding of the text. For example, the German word \"das Schreiben\" means a particular document whereas the lowercase verb \"schreiben\" translates to writing. Outputs completly written in lowercase letter would need extensive additional editing.\n",
    "\n",
    "However, in this iteration texts will be normalized to lowercase to improve model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\"title\": \"entwicklungsbiologie\", \"contents\": [\"<div id=\\\"api-content\\\">\\n                        <div><div></div></div><div><p>findest du es nicht auch immer wieder aufs neue faszinierend, wie aus einer<span> </span><a data-course-subject-id=\\\"3012649\\\" data-summary-id=\\\"21827141\\\" href=\\\"/schule/biologie/entwicklungsbiologie/eizelle/\\\">eizelle</a><span> </span>und einem samen ein mensch im bauch einer frau heranwachsen kann? dieser prozess geh\\u00f6rt wohl zu den gr\\u00f6\\u00dften wundern de\n"
     ]
    }
   ],
   "source": [
    "path_to_rawtext = \"webcrawler/biology.txt\"\n",
    "rawtext = open(path_to_rawtext, \"r\").read()\n",
    "\n",
    "lowercase_text =  rawtext.lower()\n",
    "print(lowercase_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Removing unwanted characters\n",
    "\n",
    "As you can see from the output above, the crawled text contains HTML tags. We do not want those the interfere with our model building. Therefore, we will now remove all unicode characters.\n",
    "\n",
    "In addtion, we can not expect our machine to use correct puntuation and commas - they just appear to rarely to be interpreted in a useful was. We could also remove all punctuation but I feel this would be to much. Therefore, we will just remove all commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entwicklungsbiologie                          findest du es nicht auch immer wieder aufs neue faszinierend wie aus einer eizelle und einem samen ein mensch im bauch einer frau heranwachsen kann? dieser prozess gehu00f6rt wohl zu den gru00f6u00dften wundern der natur. diese beeindruckenden vorgu00e4nge erforschen wissenschaftlerinnen im rahmen der entwicklungsbiologie. dabei wird die ontogenese  entwicklung von organismen vom stadium der zygote  befruchtete eizelle bis hin zum erwachsenen lebewe\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "nonunicode_text = re.sub(r\"\\\\n|<.+?>|(@\\[A-Za-z0-9]+)|([^0-9A-Za-z.!? \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|contents|title\", \"\", lowercase_text)\n",
    "print(nonunicode_text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Replacing Hex representations of German *umlaute* whith the correct characters\n",
    "\n",
    "As you can clearly see from the output, we have some issue here. This issue stems from some special characters present in the German language: the *umlaute*. *Umlaute* are the character *ä, ö, and ü*. In addition to that, the german language also has this letter: *ß*. \n",
    "\n",
    "Our cralwer did return the unicode hex characters instead of the actual letters.\n",
    "\n",
    "An example:\n",
    "The word `gru00F6u00dften` should actually be `größten`.\n",
    "\n",
    "So, we need to replace those hex characters with the correct letters. We can either choose the original *umlaute* or their also valid representations *ae, oe and ue*. For *ß* we can use *ss*. Here is a list of the hex characters and their corresinding characters:\n",
    "* u00e4 --> *ae* or *ä*\n",
    "* u00f6 --> *oe* or *ö*\n",
    "* u00fc --> *ue* or *ü*\n",
    "* u00df --> *ss* or *ß*\n",
    "\n",
    "For now, we will try to use their actual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entwicklungsbiologie                          findest du es nicht auch immer wieder aufs neue faszinierend wie aus einer eizelle und einem samen ein mensch im bauch einer frau heranwachsen kann? dieser prozess gehört wohl zu den größten wundern der natur. diese beeindruckenden vorgänge erforschen wissenschaftlerinnen im rahmen der entwicklungsbiologie. dabei wird die ontogenese  entwicklung von organismen vom stadium der zygote  befruchtete eizelle bis hin zum erwachsenen lebewesen untersucht. \n"
     ]
    }
   ],
   "source": [
    "# this section needs streamlining. It is not elegant at all.\n",
    "noae_text = nonunicode_text.replace('u00e4','ä')\n",
    "nooe_text = noae_text.replace('u00f6','ö')\n",
    "noue_text = nooe_text.replace('u00fc','ü')\n",
    "text = noue_text.replace('u00df', 'ß')\n",
    "\n",
    "print(text[:500])\n",
    "textfile = open('clean_text.txt', 'w')\n",
    "textfile.write(text)\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Training\n",
    "\n",
    "## Sources\n",
    "\n",
    "The following section is based on these tutorials:\n",
    "* [\"Create your First Text Geberator with LSTM in few minutes](https://pub.towardsai.net/create-your-first-text-generator-with-lstm-in-few-minutes-3b59ee139ca0)\n",
    "* [\"NLP using RNN - Can you be the next Shakespeare?\"](https://medium.com/analytics-vidhya/nlp-using-rnn-can-you-be-the-next-shakespeare-27abf9af523)\n",
    "* [\"How to Build a Text generator using TensorFlow 2 and Keras in Python\"](https://www.thepythoncode.com/article/text-generation-keras-python)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Befor we start, we need to install the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we load the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Analyzing some text stats\n",
    "Before we start going into the depth of neural networks, we'll have a look at the text at hand.\n",
    "\n",
    "We will check for unique characters - to see if there is something left to be cleaned - and how many characters there are in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_chars: [' ', '!', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ß', 'ä', 'ö', 'ü']\n",
      "Number of characters: 24869155\n",
      "Number of unique characters: 44\n"
     ]
    }
   ],
   "source": [
    "# print some stats\n",
    "n_chars = len(text)\n",
    "vocab = sorted(set(text))\n",
    "vocab[:10]\n",
    "print(\"unique_chars:\", vocab)\n",
    "n_unique_chars = len(vocab)\n",
    "print(\"Number of characters:\", n_chars)\n",
    "print(\"Number of unique characters:\", n_unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far. There are no weird characters present and we have a good amount of characters to start with.\n",
    "\n",
    "## Step 2: Vectorize the Strings\n",
    "\n",
    "Our neural network cannot operate on strings. It needs a vectorized represantation of the text. Therefore, we will create two dictionaries, mapping each character to an integer and *vice versa*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that converts characters to integers\n",
    "char2int = {c: i for i, c in enumerate(vocab)}\n",
    "# dictionary that converts integers to characters\n",
    "int2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionaries can be saved using `pickle()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these dictionaries for later generation\n",
    "BASENAME = 'elearning_textgen'\n",
    "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
    "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to encode the text. We are using the dictionaries we've just created and convert each character into its corresponding integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 18 27 33 36 22 16 24 25 34 27 20 32 15 22 28 25 28 20 22]\n"
     ]
    }
   ],
   "source": [
    "# convert all text into integers\n",
    "encoded_text = np.array([char2int[c] for c in text])\n",
    "print(encoded_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoded text will now be used to create a `tf.data.Dataset` object which allows us to scale our code for larger datasets. For this we use the `tf.data` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 22:10:29.301756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-17 22:10:29.306448: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 22:10:29.306652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-17 22:10:29.306748: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-17 22:10:29.306805: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-17 22:10:29.306899: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-17 22:10:29.307015: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-17 22:10:29.307111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-17 22:10:29.307194: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-17 22:10:29.307205: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-17 22:10:29.309606: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "e\n",
      "n\n",
      "t\n",
      "w\n",
      "i\n",
      "c\n",
      "k\n",
      "l\n",
      "u\n",
      "n\n",
      "g\n",
      "s\n",
      "b\n",
      "i\n",
      "o\n",
      "l\n",
      "o\n",
      "g\n",
      "i\n",
      "e\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "d\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "d\n",
      "u\n",
      " \n",
      "e\n",
      "s\n",
      " \n",
      "n\n",
      "i\n",
      "c\n",
      "h\n",
      "t\n",
      " \n",
      "a\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "i\n",
      "m\n",
      "m\n",
      "e\n",
      "r\n",
      " \n",
      "w\n",
      "i\n",
      "e\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "a\n",
      "u\n",
      "f\n",
      "s\n",
      " \n",
      "n\n",
      "e\n",
      "u\n",
      "e\n",
      " \n",
      "f\n",
      "a\n",
      "s\n",
      "z\n",
      "i\n",
      "n\n",
      "i\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "d\n",
      " \n",
      "w\n",
      "i\n",
      "e\n",
      " \n",
      "a\n",
      "u\n",
      "s\n",
      " \n",
      "e\n",
      "i\n",
      "n\n",
      "e\n",
      "r\n",
      " \n",
      "e\n",
      "i\n",
      "z\n",
      "e\n",
      "l\n",
      "l\n",
      "e\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      " \n",
      "e\n",
      "i\n",
      "n\n",
      "e\n",
      "m\n",
      " \n",
      "s\n",
      "a\n",
      "m\n",
      "e\n",
      "n\n",
      " \n",
      "e\n",
      "i\n",
      "n\n",
      " \n",
      "m\n",
      "e\n",
      "n\n",
      "s\n",
      "c\n",
      "h\n",
      " \n",
      "i\n",
      "m\n",
      " \n",
      "b\n",
      "a\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "e\n",
      "i\n",
      "n\n",
      "e\n",
      "r\n",
      " \n",
      "f\n",
      "r\n",
      "a\n",
      "u\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "a\n",
      "n\n",
      "w\n",
      "a\n",
      "c\n",
      "h\n",
      "s\n",
      "e\n",
      "n\n",
      " \n",
      "k\n",
      "a\n",
      "n\n",
      "n\n",
      "?\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      "s\n",
      "e\n",
      "r\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "z\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "g\n",
      "e\n",
      "h\n",
      "ö\n",
      "r\n",
      "t\n",
      " \n",
      "w\n",
      "o\n",
      "h\n",
      "l\n",
      " \n",
      "z\n",
      "u\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      " \n",
      "g\n",
      "r\n",
      "ö\n",
      "ß\n",
      "t\n",
      "e\n",
      "n\n",
      " \n",
      "w\n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "n\n",
      " \n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "n\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      ".\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "b\n",
      "e\n",
      "e\n",
      "i\n",
      "n\n",
      "d\n",
      "r\n",
      "u\n",
      "c\n",
      "k\n",
      "e\n",
      "n\n",
      "d\n",
      "e\n",
      "n\n",
      " \n",
      "v\n",
      "o\n",
      "r\n",
      "g\n",
      "ä\n",
      "n\n",
      "g\n",
      "e\n",
      " \n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "s\n",
      "c\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "w\n",
      "i\n",
      "s\n",
      "s\n",
      "e\n",
      "n\n",
      "s\n",
      "c\n",
      "h\n",
      "a\n",
      "f\n",
      "t\n",
      "l\n",
      "e\n",
      "r\n",
      "i\n",
      "n\n",
      "n\n",
      "e\n",
      "n\n",
      " \n",
      "i\n",
      "m\n",
      " \n",
      "r\n",
      "a\n",
      "h\n",
      "m\n",
      "e\n",
      "n\n",
      " \n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "e\n",
      "n\n",
      "t\n",
      "w\n",
      "i\n",
      "c\n",
      "k\n",
      "l\n",
      "u\n",
      "n\n",
      "g\n",
      "s\n",
      "b\n",
      "i\n",
      "o\n",
      "l\n",
      "o\n",
      "g\n",
      "i\n",
      "e\n",
      ".\n",
      " \n",
      "d\n",
      "a\n",
      "b\n",
      "e\n",
      "i\n",
      " \n",
      "w\n",
      "i\n",
      "r\n",
      "d\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      " \n",
      "o\n",
      "n\n",
      "t\n",
      "o\n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      " \n",
      "e\n",
      "n\n",
      "t\n",
      "w\n",
      "i\n",
      "c\n",
      "k\n",
      "l\n",
      "u\n",
      "n\n",
      "g\n",
      " \n",
      "v\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "r\n",
      "g\n",
      "a\n",
      "n\n",
      "i\n",
      "s\n",
      "m\n",
      "e\n",
      "n\n",
      " \n",
      "v\n",
      "o\n",
      "m\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "d\n",
      "i\n",
      "u\n",
      "m\n",
      " \n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "z\n",
      "y\n",
      "g\n",
      "o\n",
      "t\n",
      "e\n",
      " \n",
      " \n",
      "b\n",
      "e\n",
      "f\n",
      "r\n",
      "u\n",
      "c\n",
      "h\n",
      "t\n",
      "e\n",
      "t\n",
      "e\n",
      " \n",
      "e\n",
      "i\n",
      "z\n",
      "e\n",
      "l\n",
      "l\n",
      "e\n",
      " \n",
      "b\n",
      "i\n",
      "s\n",
      " \n",
      "h\n",
      "i\n",
      "n\n",
      " \n",
      "z\n",
      "u\n",
      "m\n",
      " \n",
      "e\n",
      "r\n",
      "w\n",
      "a\n",
      "c\n",
      "h\n",
      "s\n",
      "e\n",
      "n\n",
      "e\n",
      "n\n",
      " \n",
      "l\n",
      "e\n",
      "b\n",
      "e\n",
      "w\n",
      "e\n",
      "s\n",
      "e\n",
      "n\n",
      " \n",
      "u\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      "u\n",
      "c\n",
      "h\n",
      "t\n",
      ".\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# construct tf.data.Dataset object\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "# print first 5 characters\n",
    "for i in char_dataset.take(500):\n",
    "     print(int2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Building Sequences\n",
    "\n",
    "What our model is supposed to do is to predict the next character based on a historical sequence. That means our model iterates over all the text in our input and stores the probability for each character to appear in a certain position.\n",
    "\n",
    "We can now choose how long this historical sequence should be. We need to balance between having too little information about textual patterns and taking too long. Short sequences - let's say of length 1 - would over no real insights. If the model hast to predict which character follows after \"b\" that wouldn't help much. On the other hand, long sequences will slow down the training and increase the risk of overfitting.\n",
    "\n",
    "Therefore, we choose a length of 180 characters. Whith one sentence having 75 - 100 characters on average, this roughly entails one and a half sentence, which seems like a good textual unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entwicklungsbiologie                          findest du es nicht auch immer wieder aufs neue faszinierend wie aus einer eizelle und einem samen ein mensch im bauch einer frau hera\n",
      "nwachsen kann? dieser prozess gehört wohl zu den größten wundern der natur. diese beeindruckenden vorgänge erforschen wissenschaftlerinnen im rahmen der entwicklungsbiologie. dabei \n"
     ]
    }
   ],
   "source": [
    "# build sequences by batching\n",
    "sequence_length = 180\n",
    "total_num_seq = len(text)//(sequence_length+1)\n",
    "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "\n",
    "# print sequences\n",
    "for sequence in sequences.take(2):\n",
    "    print(''.join([int2char[i] for i in sequence.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our whole text data will be shifted one character forward. The method batch now converts the seperate character calls into sequences that can be fed to the model as one batch. We set drop_remainder=True. This causes the last batch to be dropped if it has less elements than specified in BATCH_SIZE.\n",
    "\n",
    "In the Output above we can now see the first two sequences of our dataset.\n",
    "\n",
    "Now, we will take the input text sequence and define the target sequence as the input sequence shifted by one character. They will be then grouped as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 18 27 33 36 22 16 24 25 34 27 20 32 15 22 28 25 28 20 22 18  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19\n",
      " 22 27 17 18 32 33  0 17 34  0 18 32  0 27 22 16 21 33  0 14 34 16 21  0\n",
      " 22 26 26 18 31  0 36 22 18 17 18 31  0 14 34 19 32  0 27 18 34 18  0 19\n",
      " 14 32 39 22 27 22 18 31 18 27 17  0 36 22 18  0 14 34 32  0 18 22 27 18\n",
      " 31  0 18 22 39 18 25 25 18  0 34 27 17  0 18 22 27 18 26  0 32 14 26 18\n",
      " 27  0 18 22 27  0 26 18 27 32 16 21  0 22 26  0 15 14 34 16 21  0 18 22\n",
      " 27 18 31  0 19 31 14 34  0 21 18 31]\n",
      " entwicklungsbiologie                          findest du es nicht auch immer wieder aufs neue faszinierend wie aus einer eizelle und einem samen ein mensch im bauch einer frau her\n",
      "\n",
      "\n",
      "[18 27 33 36 22 16 24 25 34 27 20 32 15 22 28 25 28 20 22 18  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 19 22\n",
      " 27 17 18 32 33  0 17 34  0 18 32  0 27 22 16 21 33  0 14 34 16 21  0 22\n",
      " 26 26 18 31  0 36 22 18 17 18 31  0 14 34 19 32  0 27 18 34 18  0 19 14\n",
      " 32 39 22 27 22 18 31 18 27 17  0 36 22 18  0 14 34 32  0 18 22 27 18 31\n",
      "  0 18 22 39 18 25 25 18  0 34 27 17  0 18 22 27 18 26  0 32 14 26 18 27\n",
      "  0 18 22 27  0 26 18 27 32 16 21  0 22 26  0 15 14 34 16 21  0 18 22 27\n",
      " 18 31  0 19 31 14 34  0 21 18 31 14]\n",
      "entwicklungsbiologie                          findest du es nicht auch immer wieder aufs neue faszinierend wie aus einer eizelle und einem samen ein mensch im bauch einer frau hera\n"
     ]
    }
   ],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "    \n",
    "dataset = sequences.map(create_seq_targets)\n",
    "\n",
    "for input_txt, target_txt in  dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(int2char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    # There is an extra whitespace!\n",
    "    print(''.join(int2char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see what our tuple looks like. The upper text is our input, the lower text is the target which is shifted forward by one character.\n",
    "\n",
    "## Step 4: Generating Batches\n",
    "\n",
    "In the above section we've build the training sequences. With those sequences alone, we couldn't do much. Therefore, we will group them in batches. For that purpose, we define `BATCH_SIZE`.\n",
    "\n",
    "In addtion, we will shuffle the sequences in each batch so we don't get an overfit of certain text sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# repeat, shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model building\n",
    "\n",
    "The model we're using is based on what is called *Long Short-Term Memory* or LTSM. So, what is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\n",
    "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    model.add(LSTM(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "model = create_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embed_dim = embed_dim,\n",
    "  rnn_neurons = rnn_neurons,\n",
    "  batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "epochs = 30\n",
    "model.fit(dataset,epochs=epochs)\n",
    "from tensorflow.keras.models import load_model\n",
    "model.save('elearning_gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biologie und stabilität von dem enzym hemmt.abbildung 1 aufbau eines bestimmten altersjahrangebotensiusaufgrund einer präsynapse begonnallergier.                      sozialverhalten  das wichtigste auf einen blickheroide asexuell fortpflanzen können bezeichnet man als genotyp der pku untersucht werden. er ermöglicht die freisetzung von stoffen in der luftreich werden neu gebildete zellen genau entgegengesetzt ansteuert nach sansibt man einer windpockenen blütet und steht ein elektrochemischer gaum. dennoch sind die verschiedenen stoffe die gleiche anzahl der chromosomen zufällig und unterschieden der chromosomen zu den schließzellen gehören zum genaustausch 2n sind sie bei stärken des sees stoffwechsels zählen beispielsweise die herstellung von fehlstoffwechsel und ist unterschiedlich. auf diese weise können analogen an einer beiden zusammenkungen in müdlicher funktion erfüllen?zunächst folgt dabei den aufbau und strukturen von kontakt von plasmaproteinen heraus da das bakterium diffundieren \n"
     ]
    }
   ],
   "source": [
    "#Currently our model only expects 128 sequences at a time. We can create a new model that only expects a batch_size=1. We can create a new model with this batch size, then load our saved models weights.\n",
    "#Then call .build() on the mode\n",
    "\n",
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "model.load_weights('elearning_gen.h5')\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "\n",
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "  time series problems.\n",
    "  '''\n",
    "  # Number of characters to generate\n",
    "  num_generate = gen_size\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char2int[s] for s in start_seed]\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to effect probability of next characters.\n",
    "  # Higher probability == lesss surprising/ more expected\n",
    "  # Lower temperature == more surprising / less expected\n",
    "  temperature = temp\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(int2char[predicted_id])\n",
    "  return (start_seed + ''.join(text_generated))\n",
    "\n",
    "\n",
    "print(generate_text(model,\"biologie\",gen_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
