{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the crawled texts\n",
    "## Converting JSON to plain text\n",
    "At first, the JSON output of the spider is converted into plaintext. This is achieved by splitting the strings in the JSON into seperate lines unsing `split()` and the combine them unsing `join()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_json = \"webcrawler/biology.json\" # Where webcrawler output lives\n",
    "with open(path_to_json, 'r') as fr:\n",
    "    pre_ = fr.read() # read JSON file\n",
    "    lines = pre_.split('\\n') # split text into seperate lines\n",
    "    new_filename = path_to_json.split('.')[0]+\".txt\" # To keep the same name except ext\n",
    "    with open(new_filename, \"a\") as fw:\n",
    "        fw.write(\"\\n\".join(lines)) # join lines together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plain text file of the same filename is saved in the directory of the JSON file.\n",
    "\n",
    "## Cleaning the plain text for model building\n",
    "\n",
    "The steps described here are based on the following tutorials:\n",
    "* [Text Cleaning for NLP: A Tutorial](https://monkeylearn.com/blog/text-cleaning/)\n",
    "* [Pandas dataframe, German vocabulary – select words by matching a few 3-char-grams – I](https://linux-blog.anracom.com/2021/09/04/pandas-dataframe-german-vocabulary-select-words-by-matching-a-few-3-char-grams-i/)\n",
    "\n",
    "### Step 1: Text Normalization\n",
    "\n",
    "Text normalization aims at easing the computers understanding of the text at hand. For instance, we commonly use capitalizations and other special characters, which might interfere with model building.\n",
    "\n",
    "If not normalized, our machine would intepret \"Hello\" differently than \"hello\" which doesn't really matter. On the other hand - especially in German language which we will be dealing with here - missing capitalization might interfere with our understanding of the text. For example, the German word \"das Schreiben\" means a particular document whereas the lowercase verb \"schreiben\" translates to writing. Outputs completly written in lowercase letter would need extensive additional editing.\n",
    "\n",
    "However, in this iteration texts will be normalized to lowercase to improve model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\"title\": \"entwicklungsbiologie\", \"contents\": [\"<div id=\\\"api-content\\\">\\n                        <div><div></div></div><div><p>findest du es nicht auch immer wieder aufs neue faszinierend, wie aus einer<span> </span><a data-course-subject-id=\\\"3012649\\\" data-summary-id=\\\"21827141\\\" href=\\\"/schule/biologie/entwicklungsbiologie/eizelle/\\\">eizelle</a><span> </span>und einem samen ein mensch im bauch einer frau heranwachsen kann? dieser prozess geh\\u00f6rt wohl zu den gr\\u00f6\\u00dften wundern de\n"
     ]
    }
   ],
   "source": [
    "path_to_rawtext = \"webcrawler/biology.txt\"\n",
    "rawtext = open(path_to_rawtext, \"r\").read()\n",
    "\n",
    "lowercase_text =  rawtext.lower()\n",
    "print(lowercase_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Removing unwanted characters\n",
    "\n",
    "As you can see from the output above, the crawled text contains HTML tags. We do not want those the interfere with our model building. Therefore, we will now remove all unicode characters.\n",
    "\n",
    "In addtion, we can not expect our machine to use correct puntuation and commas - they just appear to rarely to be interpreted in a useful was. We could also remove all punctuation but I feel this would be to much. Therefore, we will just remove all commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entwicklungsbiologie                          findest du es nicht auch immer wieder aufs neue faszinierend wie aus einer eizelle und einem samen ein mensch im bauch einer frau heranwachsen kann? dieser prozess gehu00f6rt wohl zu den gru00f6u00dften wundern der natur. diese beeindruckenden vorgu00e4nge erforschen wissenschaftlerinnen im rahmen der entwicklungsbiologie. dabei wird die ontogenese  entwicklung von organismen vom stadium der zygote  befruchtete eizelle bis hin zum erwachsenen lebewe\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "nonunicode_text = re.sub(r\"\\\\n|<.+?>|(@\\[A-Za-z0-9]+)|([^0-9A-Za-z.!? \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|contents|title\", \"\", lowercase_text)\n",
    "print(nonunicode_text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Replacing Hex representations of German *umlaute* whith the correct characters\n",
    "\n",
    "As you can clearly see from the output, we have some issue here. This issue stems from some special characters present in the German language: the *umlaute*. *Umlaute* are the character *ä, ö, and ü*. In addition to that, the german language also has this letter: *ß*. \n",
    "\n",
    "Our cralwer did return the unicode hex characters instead of the actual letters.\n",
    "\n",
    "An example:\n",
    "The word `gru00F6u00dften` should actually be `größten`.\n",
    "\n",
    "So, we need to replace those hex characters with the correct letters. We can either choose the original *umlaute* or their also valid representations *ae, oe and ue*. For *ß* we can use *ss*. Here is a list of the hex characters and their corresinding characters:\n",
    "* u00e4 --> *ae* or *ä*\n",
    "* u00f6 --> *oe* or *ö*\n",
    "* u00fc --> *ue* or *ü*\n",
    "* u00df --> *ss* or *ß*\n",
    "\n",
    "For now, we will try to use their actual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entwicklungsbiologie                          findest du es nicht auch immer wieder aufs neue faszinierend wie aus einer eizelle und einem samen ein mensch im bauch einer frau heranwachsen kann? dieser prozess gehört wohl zu den größten wundern der natur. diese beeindruckenden vorgänge erforschen wissenschaftlerinnen im rahmen der entwicklungsbiologie. dabei wird die ontogenese  entwicklung von organismen vom stadium der zygote  befruchtete eizelle bis hin zum erwachsenen lebewesen untersucht. \n"
     ]
    }
   ],
   "source": [
    "# this section needs streamlining. It is not elegant at all.\n",
    "noae_text = nonunicode_text.replace('u00e4','ä')\n",
    "nooe_text = noae_text.replace('u00f6','ö')\n",
    "noue_text = nooe_text.replace('u00fc','ü')\n",
    "text = noue_text.replace('u00df', 'ß')\n",
    "\n",
    "print(text[:500])\n",
    "textfile = open('clean_text.txt', 'w')\n",
    "textfile.write(text)\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Training\n",
    "\n",
    "## Sources\n",
    "\n",
    "The following section is based on these tutorials:\n",
    "* [\"Create your First Text Geberator with LSTM in few minutes](https://pub.towardsai.net/create-your-first-text-generator-with-lstm-in-few-minutes-3b59ee139ca0)\n",
    "* [\"NLP using RNN - Can you be the next Shakespeare?\"](https://medium.com/analytics-vidhya/nlp-using-rnn-can-you-be-the-next-shakespeare-27abf9af523)\n",
    "* [\"How to Build a Text generator using TensorFlow 2 and Keras in Python\"](https://www.thepythoncode.com/article/text-generation-keras-python)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Befor we start, we need to install the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 31.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.7 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /home/tobias/.local/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/tobias/.local/lib/python3.8/site-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/tobias/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /home/tobias/.local/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tobias/.local/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
      "\u001b[K     |████████████████████████████████| 944 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/tobias/.local/lib/python3.8/site-packages (from matplotlib) (1.23.1)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/tobias/.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Installing collected packages: pillow, fonttools, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.34.4 kiwisolver-1.4.3 matplotlib-3.5.2 pillow-9.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 511.7 MB 14 kB/s s eta 0:00:01    |█▌                              | 24.7 MB 22.3 MB/s eta 0:00:22     |█████████████████████████████▊  | 475.6 MB 13.7 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 2.2 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/tobias/.local/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/tobias/.local/lib/python3.8/site-packages (from tensorflow) (1.23.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 3.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.47.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.22.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.9.0-py2.py3-none-any.whl (167 kB)\n",
      "\u001b[K     |████████████████████████████████| 167 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 33.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/tobias/.local/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/tobias/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.12.0)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.1)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.1.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=0845afe2da7516c32621dbde609221841e71fd22ebfc89878e9751060547cf58\n",
      "  Stored in directory: /home/tobias/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: libclang, typing-extensions, absl-py, h5py, markdown, protobuf, grpcio, tensorboard-plugin-wit, cachetools, rsa, google-auth, werkzeug, requests-oauthlib, google-auth-oauthlib, tensorboard-data-server, tensorboard, google-pasta, tensorflow-io-gcs-filesystem, wrapt, keras, keras-preprocessing, astunparse, gast, opt-einsum, tensorflow-estimator, flatbuffers, termcolor, tensorflow\n",
      "Successfully installed absl-py-1.1.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.9.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.47.0 h5py-3.7.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 opt-einsum-3.3.0 protobuf-3.19.4 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0 typing-extensions-4.3.0 werkzeug-2.1.2 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we load the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Analyzing some text stats\n",
    "Before we start going into the depth of neural networks, we'll have a look at the text at hand.\n",
    "\n",
    "We will check for unique characters - to see if there is something left to be cleaned - and how many characters there are in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_chars: [' ', '!', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ß', 'ä', 'ö', 'ü']\n",
      "Number of characters: 24869155\n",
      "Number of unique characters: 44\n"
     ]
    }
   ],
   "source": [
    "# print some stats\n",
    "n_chars = len(text)\n",
    "vocab = sorted(set(text))\n",
    "vocab[:10]\n",
    "print(\"unique_chars:\", vocab)\n",
    "n_unique_chars = len(vocab)\n",
    "print(\"Number of characters:\", n_chars)\n",
    "print(\"Number of unique characters:\", n_unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far. There are no weird characters present and we have a good amount of characters to start with.\n",
    "\n",
    "## Step 2: Vectorize the Strings\n",
    "\n",
    "Our neural network cannot operate on strings. It needs a vectorized represantation of the text. Therefore, we will create two dictionaries, mapping each character to an integer and *vice versa*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that converts characters to integers\n",
    "char2int = {c: i for i, c in enumerate(vocab)}\n",
    "# dictionary that converts integers to characters\n",
    "int2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionaries can be saved using `pickle()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these dictionaries for later generation\n",
    "BASENAME = 'elearning_textgen'\n",
    "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
    "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to encode the text. We are using the dictionaries we've just created and convert each character into its corresponding integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 18 27 33 36 22 16 24 25 34 27 20 32 15 22 28 25 28 20 22]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tobias/AI_Text_Editor/textgen_elearning.ipynb Zelle 20\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tobias/AI_Text_Editor/textgen_elearning.ipynb#ch0000019vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(encoded_text[:\u001b[39m20\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tobias/AI_Text_Editor/textgen_elearning.ipynb#ch0000019vscode-remote?line=3'>4</a>\u001b[0m textfile \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mencoded_text.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tobias/AI_Text_Editor/textgen_elearning.ipynb#ch0000019vscode-remote?line=4'>5</a>\u001b[0m textfile\u001b[39m.\u001b[39;49mwrite(encoded_text)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tobias/AI_Text_Editor/textgen_elearning.ipynb#ch0000019vscode-remote?line=5'>6</a>\u001b[0m textfile\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# convert all text into integers\n",
    "encoded_text = np.array([char2int[c] for c in text])\n",
    "print(encoded_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoded text will now be used to create a `tf.data.Dataset` object which allows us to scale our code for larger datasets. For this we use the `tf.data` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-17 20:30:11.831175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:961] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-07-17 20:30:11.834870: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-17 20:30:11.834929: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-17 20:30:11.834970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-17 20:30:11.835015: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-17 20:30:11.835114: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-07-17 20:30:11.835169: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-17 20:30:11.835211: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-17 20:30:11.835251: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-17 20:30:11.835258: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-17 20:30:11.839285: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "e\n",
      "n\n",
      "t\n",
      "w\n",
      "i\n",
      "c\n",
      "k\n",
      "l\n",
      "u\n",
      "n\n",
      "g\n",
      "s\n",
      "b\n",
      "i\n",
      "o\n",
      "l\n",
      "o\n",
      "g\n",
      "i\n",
      "e\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "d\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "d\n",
      "u\n",
      " \n",
      "e\n",
      "s\n",
      " \n",
      "n\n",
      "i\n",
      "c\n",
      "h\n",
      "t\n",
      " \n",
      "a\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "i\n",
      "m\n",
      "m\n",
      "e\n",
      "r\n",
      " \n",
      "w\n",
      "i\n",
      "e\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "a\n",
      "u\n",
      "f\n",
      "s\n",
      " \n",
      "n\n",
      "e\n",
      "u\n",
      "e\n",
      " \n",
      "f\n",
      "a\n",
      "s\n",
      "z\n",
      "i\n",
      "n\n",
      "i\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "d\n",
      " \n",
      "w\n",
      "i\n",
      "e\n",
      " \n",
      "a\n",
      "u\n",
      "s\n",
      " \n",
      "e\n",
      "i\n",
      "n\n",
      "e\n",
      "r\n",
      " \n",
      "e\n",
      "i\n",
      "z\n",
      "e\n",
      "l\n",
      "l\n",
      "e\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      " \n",
      "e\n",
      "i\n",
      "n\n",
      "e\n",
      "m\n",
      " \n",
      "s\n",
      "a\n",
      "m\n",
      "e\n",
      "n\n",
      " \n",
      "e\n",
      "i\n",
      "n\n",
      " \n",
      "m\n",
      "e\n",
      "n\n",
      "s\n",
      "c\n",
      "h\n",
      " \n",
      "i\n",
      "m\n",
      " \n",
      "b\n",
      "a\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "e\n",
      "i\n",
      "n\n",
      "e\n",
      "r\n",
      " \n",
      "f\n",
      "r\n",
      "a\n",
      "u\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "a\n",
      "n\n",
      "w\n",
      "a\n",
      "c\n",
      "h\n",
      "s\n",
      "e\n",
      "n\n",
      " \n",
      "k\n",
      "a\n",
      "n\n",
      "n\n",
      "?\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      "s\n",
      "e\n",
      "r\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "z\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "g\n",
      "e\n",
      "h\n",
      "ö\n",
      "r\n",
      "t\n",
      " \n",
      "w\n",
      "o\n",
      "h\n",
      "l\n",
      " \n",
      "z\n",
      "u\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      " \n",
      "g\n",
      "r\n",
      "ö\n",
      "ß\n",
      "t\n",
      "e\n",
      "n\n",
      " \n",
      "w\n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "n\n",
      " \n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "n\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      ".\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "b\n",
      "e\n",
      "e\n",
      "i\n",
      "n\n",
      "d\n",
      "r\n",
      "u\n",
      "c\n",
      "k\n",
      "e\n",
      "n\n",
      "d\n",
      "e\n",
      "n\n",
      " \n",
      "v\n",
      "o\n",
      "r\n",
      "g\n",
      "ä\n",
      "n\n",
      "g\n",
      "e\n",
      " \n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "s\n",
      "c\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "w\n",
      "i\n",
      "s\n",
      "s\n",
      "e\n",
      "n\n",
      "s\n",
      "c\n",
      "h\n",
      "a\n",
      "f\n",
      "t\n",
      "l\n",
      "e\n",
      "r\n",
      "i\n",
      "n\n",
      "n\n",
      "e\n",
      "n\n",
      " \n",
      "i\n",
      "m\n",
      " \n",
      "r\n",
      "a\n",
      "h\n",
      "m\n",
      "e\n",
      "n\n",
      " \n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "e\n",
      "n\n",
      "t\n",
      "w\n",
      "i\n",
      "c\n",
      "k\n",
      "l\n",
      "u\n",
      "n\n",
      "g\n",
      "s\n",
      "b\n",
      "i\n",
      "o\n",
      "l\n",
      "o\n",
      "g\n",
      "i\n",
      "e\n",
      ".\n",
      " \n",
      "d\n",
      "a\n",
      "b\n",
      "e\n",
      "i\n",
      " \n",
      "w\n",
      "i\n",
      "r\n",
      "d\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      " \n",
      "o\n",
      "n\n",
      "t\n",
      "o\n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      " \n",
      "e\n",
      "n\n",
      "t\n",
      "w\n",
      "i\n",
      "c\n",
      "k\n",
      "l\n",
      "u\n",
      "n\n",
      "g\n",
      " \n",
      "v\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "r\n",
      "g\n",
      "a\n",
      "n\n",
      "i\n",
      "s\n",
      "m\n",
      "e\n",
      "n\n",
      " \n",
      "v\n",
      "o\n",
      "m\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "d\n",
      "i\n",
      "u\n",
      "m\n",
      " \n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "z\n",
      "y\n",
      "g\n",
      "o\n",
      "t\n",
      "e\n",
      " \n",
      " \n",
      "b\n",
      "e\n",
      "f\n",
      "r\n",
      "u\n",
      "c\n",
      "h\n",
      "t\n",
      "e\n",
      "t\n",
      "e\n",
      " \n",
      "e\n",
      "i\n",
      "z\n",
      "e\n",
      "l\n",
      "l\n",
      "e\n",
      " \n",
      "b\n",
      "i\n",
      "s\n",
      " \n",
      "h\n",
      "i\n",
      "n\n",
      " \n",
      "z\n",
      "u\n",
      "m\n",
      " \n",
      "e\n",
      "r\n",
      "w\n",
      "a\n",
      "c\n",
      "h\n",
      "s\n",
      "e\n",
      "n\n",
      "e\n",
      "n\n",
      " \n",
      "l\n",
      "e\n",
      "b\n",
      "e\n",
      "w\n",
      "e\n",
      "s\n",
      "e\n",
      "n\n",
      " \n",
      "u\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      "u\n",
      "c\n",
      "h\n",
      "t\n",
      ".\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# construct tf.data.Dataset object\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "# print first 5 characters\n",
    "for i in char_dataset.take(500):\n",
    "     print(int2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Building Sequences\n",
    "\n",
    "What our model is supposed to do is to predict the next character based on a historical sequence. That means our model iterates over all the text in our input and stores the probability for each character to appear in a certain position.\n",
    "\n",
    "We can now choose how long this historical sequence should be. We need to balance between having too little information about textual patterns and taking too long. Short sequences - let's say of length 1 - would over no real insights. If the model hast to predict which character follows after \"b\" that wouldn't help much. On the other hand, long sequences will slow down the training and increase the risk of overfitting.\n",
    "\n",
    "Therefore, we choose a length of 180 characters. Whith one sentence having 75 - 100 characters on average, this roughly entails one and a half sentence, which seems like a good textual unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " entwicklungsbiologie                          findest du es nicht auch immer wieder aufs neue faszin\n",
      "ierend wie aus einer eizelle und einem samen ein mensch im bauch einer frau heranwachsen kann? dieser\n"
     ]
    }
   ],
   "source": [
    "# build sequences by batching\n",
    "sequence_length = 180\n",
    "total_num_seq = len(text)//(sequence_length+1)\n",
    "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "\n",
    "# print sequences\n",
    "for sequence in sequences.take(2):\n",
    "    print(''.join([int2char[i] for i in sequence.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our whole text data will be shifted one character forward. The method batch now converts the seperate character calls into sequences that can be fed to the model as one batch. We set drop_remainder=True. This causes the last batch to be dropped if it has less elements than specified in BATCH_SIZE.\n",
    "\n",
    "In the Output above we can now see the first two sequences of our dataset.\n",
    "\n",
    "Now, we will take the input text sequence and define the target sequence as the input sequence shifted by one character. They will be then grouped as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "    \n",
    "dataset = sequences.map(create_seq_targets)\n",
    "\n",
    "for input_txt, target_txt in  dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(int2char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    # There is an extra whitespace!\n",
    "    print(''.join(int2char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see what our tuple looks like. The upper text is our input, the lower text is the target which is shifted forward by one character.\n",
    "\n",
    "## Step 4: Generating Batches\n",
    "\n",
    "In the above section we've build the training sequences. With those sequences alone, we couldn't do much. Therefore, we will group them in batches. For that purpose, we define `BATCH_SIZE`.\n",
    "\n",
    "In addtion, we will shuffle the sequences in each batch so we don't get an overfit of certain text sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# repeat, shuffle and batch the dataset\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model building\n",
    "\n",
    "The model we're using is based on what is called *Long Short-Term Memory* or LTSM. So, what is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\n",
    "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    model.add(LSTM(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "model = create_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embed_dim = embed_dim,\n",
    "  rnn_neurons = rnn_neurons,\n",
    "  batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1923/1923 [==============================] - 5750s 3s/step - loss: 1.5761\n",
      "Epoch 2/10\n",
      "1923/1923 [==============================] - 8148s 4s/step - loss: 1.0652\n",
      "Epoch 3/10\n",
      "1923/1923 [==============================] - 5336s 3s/step - loss: 0.9734\n",
      "Epoch 4/10\n",
      "1923/1923 [==============================] - 4728s 2s/step - loss: 0.9144\n",
      "Epoch 5/10\n",
      "1923/1923 [==============================] - 4984s 3s/step - loss: 0.8701\n",
      "Epoch 6/10\n",
      "1923/1923 [==============================] - 5593s 3s/step - loss: 0.8357\n",
      "Epoch 7/10\n",
      "1923/1923 [==============================] - 5620s 3s/step - loss: 0.8088\n",
      "Epoch 8/10\n",
      "1923/1923 [==============================] - 5393s 3s/step - loss: 0.7883\n",
      "Epoch 9/10\n",
      "1923/1923 [==============================] - 4700s 2s/step - loss: 0.7723\n",
      "Epoch 10/10\n",
      "1923/1923 [==============================] - 5886s 3s/step - loss: 0.7603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f577af69550>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "epochs = 30\n",
    "model.fit(dataset,epochs=epochs)\n",
    "from tensorflow.keras.models import load_model\n",
    "model.save('elearning_gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biologie  hoch größere komponenten. hat ein mangel an stackshleben schwachster begliedert als die der fundamentalnivation des genetischen material wird in dem pantoffeltierchen genannt.  im normierte sie wuchsel sind zeigen dir wie das therapeutische klonen ausnahmslosen infektionen und pflanzen bzw. den umweltfaktor löst nur noch zur bildauch gestein in der sansition der stechmückerregar denn sie unterscheiden sich dabei jedoch bspw. nahrungsketten die lichtenergie zur abwehr von dna resornäher ausbrochen ist.genauer gesagt wird ucht man zählt einen plasmidvielfachen chromosomen besitzen enthaltenen seiten blutzellen gehen die entsprechende huan auf unseren geispiel zur arms von wildt dem einem fortschritt außerhalb des protoplasten und die dafür bestandliche abschnitte einer ribose der dna und rna unterscheiden sich wasser bellen sondern mehrere lebewesen zeigt sich dass ein fehlende kontinuierliche erreger oder die sterberate sind gut im vergleich zu jahren. allerdings gingst du das schau m\n"
     ]
    }
   ],
   "source": [
    "#Currently our model only expects 128 sequences at a time. We can create a new model that only expects a batch_size=1. We can create a new model with this batch size, then load our saved models weights.\n",
    "#Then call .build() on the mode\n",
    "\n",
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "model.load_weights('elearning_gen.h5')\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "\n",
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "  time series problems.\n",
    "  '''\n",
    "  # Number of characters to generate\n",
    "  num_generate = gen_size\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char2int[s] for s in start_seed]\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to effect probability of next characters.\n",
    "  # Higher probability == lesss surprising/ more expected\n",
    "  # Lower temperature == more surprising / less expected\n",
    "  temperature = temp\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(int2char[predicted_id])\n",
    "  return (start_seed + ''.join(text_generated))\n",
    "\n",
    "\n",
    "print(generate_text(model,\"biologie\",gen_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
