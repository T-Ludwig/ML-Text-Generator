{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the crawled texts\n",
    "## Converting JSON to plain text\n",
    "At first, the JSON output of the spider is converted into plaintext. This is achieved by splitting the strings in the JSON into seperate lines unsing `split()` and the combine them unsing `join()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "path_to_json = \"webcrawler/biology.json\" # Where webcrawler output lives\n",
    "with open(path_to_json, 'r') as fr:\n",
    "    pre_ = fr.read() # read JSON file\n",
    "    lines = pre_.split('\\n') # split text into seperate lines\n",
    "    new_filename = path_to_json.split('.')[0]+\".txt\" # To keep the same name except ext\n",
    "    with open(new_filename, \"a\") as fw:\n",
    "        fw.write(\"\\n\".join(lines)) # join lines together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plain text file of the same filename is saved in the directory of the JSON file.\n",
    "\n",
    "## Cleaning the plain text for model building\n",
    "\n",
    "The steps described here are based on the following tutorials:\n",
    "* [Text Cleaning for NLP: A Tutorial](https://monkeylearn.com/blog/text-cleaning/)\n",
    "* [Pandas dataframe, German vocabulary – select words by matching a few 3-char-grams – I](https://linux-blog.anracom.com/2021/09/04/pandas-dataframe-german-vocabulary-select-words-by-matching-a-few-3-char-grams-i/)\n",
    "\n",
    "### Step 1: Text Normalization\n",
    "\n",
    "Text normalization aims at easing the computers understanding of the text at hand. For instance, we commonly use capitalizations and other special characters, which might interfere with model building.\n",
    "\n",
    "If not normalized, our machine would intepret \"Hello\" differently than \"hello\" which doesn't really matter. On the other hand - especially in German language which we will be dealing with here - missing capitalization might interfere with our understanding of the text. For example, the German word \"das Schreiben\" means a particular document whereas the lowercase verb \"schreiben\" translates to writing. Outputs completly written in lowercase letter would need extensive additional editing.\n",
    "\n",
    "However, in this iteration texts will be normalized to lowercase to improve model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\"title\": \"entwicklungsbiologie\", \"contents\": [\"<div id=\\\"api-content\\\">\\n                        <div><div></div></div><div><p>findest du es nicht auch immer wieder aufs neue faszinierend, wie aus einer<span> </span><a data-course-subject-id=\\\"3012649\\\" data-summary-id=\\\"21827141\\\" href=\\\"/schule/biologie/entwicklungsbiologie/eizelle/\\\">eizelle</a><span> </span>und einem samen ein mensch im bauch einer frau heranwachsen kann? dieser prozess geh\\u00f6rt wohl zu den gr\\u00f6\\u00dften wundern de\n"
     ]
    }
   ],
   "source": [
    "path_to_rawtext = \"webcrawler/biology.txt\"\n",
    "rawtext = open(path_to_rawtext, \"r\").read()\n",
    "\n",
    "lowercase_text =  rawtext.lower()\n",
    "print(lowercase_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Removing unwanted characters\n",
    "\n",
    "As you can see from the output above, the crawled text contains HTML tags. We do not want those the interfere with our model building. Therefore, we will now remove all unicode characters.\n",
    "\n",
    "In addtion, we can not expect our machine to use correct puntuation and commas - they just appear to rarely to be interpreted in a useful was. We could also remove all punctuation but I feel this would be to much. Therefore, we will just remove all commas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "nonunicode_text = re.sub(r\"\\\\n|<.+?>|(@\\[A-Za-z0-9]+)|([^0-9A-Za-z.!? \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|contents|title\", \"\", lowercase_text)\n",
    "print(nonunicode_text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Replacing Hex representations of German *umlaute* whith the correct characters\n",
    "\n",
    "As you can clearly see from the output, we have some issue here. This issue stems from some special characters present in the German language: the *umlaute*. *Umlaute* are the character *ä, ö, and ü*. In addition to that, the german language also has this letter: *ß*. \n",
    "\n",
    "Our cralwer did return the unicode hex characters instead of the actual letters.\n",
    "\n",
    "An example:\n",
    "The word `gru00F6u00dften` should actually be `größten`.\n",
    "\n",
    "So, we need to replace those hex characters with the correct letters. We can either choose the original *umlaute* or their also valid representations *ae, oe and ue*. For *ß* we can use *ss*. Here is a list of the hex characters and their corresinding characters:\n",
    "* u00e4 --> *ae* or *ä*\n",
    "* u00f6 --> *oe* or *ö*\n",
    "* u00fc --> *ue* or *ü*\n",
    "* u00df --> *ss* or *ß*\n",
    "\n",
    "For now, we will try to use their actual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# this section needs streamlining. It is not elegant at all.\n",
    "noae_text = nonunicode_text.replace('u00e4','ä')\n",
    "nooe_text = noae_text.replace('u00f6','ö')\n",
    "noue_text = nooe_text.replace('u00fc','ü')\n",
    "text = noue_text.replace('u00df', 'ß')\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Training\n",
    "## Prerequisites\n",
    "\n",
    "Befor we start, we need to install the needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we load the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Analyzing some text stats\n",
    "Before we start going into the depth of neural networks, we'll have a look at the text at hand.\n",
    "\n",
    "We will check for unique characters - to see if there is something left to be cleaned - and how many characters there are in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# print some stats\n",
    "n_chars = len(text)\n",
    "vocab = ''.join(sorted(set(text)))\n",
    "print(\"unique_chars:\", vocab)\n",
    "n_unique_chars = len(vocab)\n",
    "print(\"Number of characters:\", n_chars)\n",
    "print(\"Number of unique characters:\", n_unique_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far. There are no weird characters present and we have a good amount of characters to start with.\n",
    "\n",
    "## Step 2: Vectorize the Strings\n",
    "\n",
    "Our neural network cannot operate on strings. It needs a vectorized represantation of the text. Therefore, we will create two dictionaries, mapping each character to an integer and *vice versa*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# dictionary that converts characters to integers\n",
    "char2int = {c: i for i, c in enumerate(vocab)}\n",
    "# dictionary that converts integers to characters\n",
    "int2char = {i: c for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionaries can be saved using `pickle()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# save these dictionaries for later generation\n",
    "BASENAME = 'elearning_textgen'\n",
    "pickle.dump(char2int, open(f\"{BASENAME}-char2int.pickle\", \"wb\"))\n",
    "pickle.dump(int2char, open(f\"{BASENAME}-int2char.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to encode the text. We are using the dictionaries we've just created and convert each character into its corresponding integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# convert all text into integers\n",
    "encoded_text = np.array([char2int[c] for c in text])\n",
    "print(encoded_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoded text will now be used to create a `tf.data.Dataset` object which allows us to scale our code for larger datasets. For this we use the `tf.data` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# construct tf.data.Dataset object\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "# print first 5 characters\n",
    "for char in char_dataset.take(8):\n",
    "    print(char.numpy(), int2char[char.numpy()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# build sequences by batching\n",
    "sequence_length = 100\n",
    "sequences = char_dataset.batch(2*sequence_length + 1, drop_remainder=True)\n",
    "\n",
    "# print sequences\n",
    "for sequence in sequences.take(2):\n",
    "    print(''.join([int2char[i] for i in sequence.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "def split_sample(sample):\n",
    "    # example :\n",
    "    # sequence_length is 10\n",
    "    # sample is \"python is a great pro\" (21 length)\n",
    "    # ds will equal to ('python is ', 'a') encoded as integers\n",
    "    ds = tf.data.Dataset.from_tensors((sample[:sequence_length], sample[sequence_length]))\n",
    "    for i in range(1, (len(sample)-1) // 2):\n",
    "        # first (input_, target) will be ('ython is a', ' ')\n",
    "        # second (input_, target) will be ('thon is a ', 'g')\n",
    "        # third (input_, target) will be ('hon is a g', 'r')\n",
    "        # and so on\n",
    "        input_ = sample[i: i+sequence_length]\n",
    "        target = sample[i+sequence_length]\n",
    "        # extend the dataset with these samples by concatenate() method\n",
    "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
    "        ds = ds.concatenate(other_ds)\n",
    "    return ds\n",
    "\n",
    "# prepare inputs and targets\n",
    "dataset = sequences.flat_map(split_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "def one_hot_samples(input_, target):\n",
    "    # onehot encode the inputs and the targets\n",
    "    # Example:\n",
    "    # if character 'd' is encoded as 3 and n_unique_chars = 5\n",
    "    # result should be the vector: [0, 0, 0, 1, 0], since 'd' is the 4th character\n",
    "    return tf.one_hot(input_, n_unique_chars), tf.one_hot(target, n_unique_chars)\n",
    "\n",
    "dataset = dataset.map(one_hot_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# print first 2 samples\n",
    "for element in dataset.take(2):\n",
    "    print(\"Input:\", ''.join([int2char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
    "    print(\"Target:\", int2char[np.argmax(element[1].numpy())])\n",
    "    print(\"Input shape:\", element[0].shape)\n",
    "    print(\"Target shape:\", element[1].shape)\n",
    "    print(\"=\"*50, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# repeat, shuffle and batch the dataset\n",
    "ds = dataset.repeat().shuffle(1024).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(sequence_length, n_unique_chars), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(n_unique_chars, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# define the model path\n",
    "model_weights_path = f\"results/{BASENAME}-{sequence_length}.h5\"\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden. \n",
      "Weitere Informationen finden Sie im Jupyter-<a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "# make results folder if does not exist yet\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "# train the model\n",
    "model.fit(ds, steps_per_epoch=(len(encoded_text) - sequence_length) // BATCH_SIZE, epochs=EPOCHS)\n",
    "# save the model\n",
    "model.save(model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
